import os
import streamlit as st
from dotenv import load_dotenv

# Must be the first Streamlit command
st.set_page_config(page_title="âš–ï¸ Indian Legal Chatbot", layout="centered")
st.title("ğŸ§‘â€âš–ï¸ AI Legal Assistant (India)")
# ENV loading
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")

# Optional debug: confirm key
st.write("ğŸ”‘Â GROQ key loaded:", GROQ_API_KEY[:6] + "â€¦" if GROQ_API_KEY else "âŒÂ MISSING")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Lazyâ€‘load heavy stuff
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#@st.cache_resource(show_spinner=False)
def load_retriever():
    from sentence_transformers import SentenceTransformer
    import faiss, pickle

    #model = SentenceTransformer("all-MiniLM-L6-v2")
    model = SentenceTransformer("paraphrase-MiniLM-L6-v2")

    index = faiss.read_index("retriever/legal_index.faiss")
    with open("retriever/legal_docs.pkl", "rb") as f:
        docs = pickle.load(f)

    return model, index, docs


@st.cache_resource(show_spinner=False)
def load_groq():
    from groq import Groq
    return Groq(api_key=GROQ_API_KEY)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Helper functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def retrieve_context(model, index, docs, query, k=3):
    emb = model.encode([query])
    _, idxs = index.search(emb, k)
    return [docs[i] for i in idxs[0]]


def generate_answer(client, context_chunks, question):
    ctx = "\n\n".join(context_chunks)
    prompt = (
        "You are a legal assistant. Answer the Indian legal question "
        "using ONLY the context below.\n\n"
        f"Context:\n{ctx}\n\nQuestion: {question}"
    )

    resp = client.chat.completions.create(
        model="llama3-8b-8192",     # small & fast
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=512,
    )
    return resp.choices[0].message.content.strip()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
query = st.text_input("Enter your legal question:")

if query:
    st.info("âš™ï¸â€¯Model loading the first time may take ~10â€¯secâ€¦")
    with st.spinner("Thinking like a lawyerâ€¦"):
        model, index, docs = load_retriever()
        client = load_groq()

        chunks = retrieve_context(model, index, docs, query)
        answer = generate_answer(client, chunks, query)

    # â”€â”€â”€â”€â”€ results â”€â”€â”€â”€â”€
    st.subheader("ğŸ’¬ AI Answer")
    st.write(answer)

    st.subheader("ğŸ“š Retrieved Context")
    for i, ch in enumerate(chunks, 1):
        st.markdown(f"**{i}.** {ch.strip()}")

    st.caption("âš ï¸Â This is informational only and **not** legal advice.")
